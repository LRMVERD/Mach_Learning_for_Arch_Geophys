{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d4c2e0-f758-4192-b280-8e5cdbb84548",
   "metadata": {},
   "source": [
    "# Please run the cells by pressing Shift + Enter for each cell. \n",
    "First import the necesseary libraries and the napari viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "731d0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import vigra\n",
    "import napari\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cv2 as cv\n",
    "import geopandas as gpd\n",
    "import pandas\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import tifffile\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage import filters, morphology, transform as transf\n",
    "from scipy import ndimage as ndi, signal\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.affinity import translate, scale, affine_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3559f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the GUI and start the napari viewer\n",
    "%matplotlib qt\n",
    "%gui qt\n",
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94caf84",
   "metadata": {},
   "source": [
    "# 1. Import geophysical image \n",
    "Import the geophysical image to be analysed into the napari viewer using 'File -> Open File(s)', and then run the cell below. If you would like to open a 3D dataset consisting of several horizontal slices, create a 3D file containing the entire volume (e.g. a 3D Tiff) or use the option 'File -> Open Files as Stack'. This will allow you to select all horizontal slice images at once, and import the complete volume in napari. 3D data can be visualised as horizontal slices, vertical slices (xz and yz plane) and as a volume; use the second and third button in the lower left corner of the viewer.<br> Run the code below, to convert the input data to a numpy array, extract the path of the input data and determine the number of dimensions for late use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84fe2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageOrig = viewer.layers[0].data\n",
    "image = np.array(imageOrig)\n",
    "path = viewer.layers[0].source.path\n",
    "nrofdim = np.ndim(imageOrig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a2d92b",
   "metadata": {},
   "source": [
    "# 2. Machine learning based pixel classification\n",
    "Pixel classification using the code below will usually be slower than in ilastik or other segmentation tools such as LABKIT or Trainable Weka Segmentation. For 3D data in particular, run times may be significantly longer and RAM usage may be significantly larger. If pixel segmentation has been performed by means of an external tool such as ilastik, please import the segmented image in step 3.1 below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc44403-b2cd-4005-bdf1-eb1597a51857",
   "metadata": {},
   "source": [
    "## 2.1. Load features\n",
    "If no pixel features have been calculated for your input data, please go to step 2.2.<br>\n",
    "If pixel features, feature names and scales for the input data have previously been saved, they can be loaded here. Please adjust the file name in the first line of each cell if necessary. The files should be in the same folder as the Jupyter Notebook, or their path should be specified in the code below. After loading the features, please go immediately to step 2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68b8bbb1-04e7-4edf-853b-0c65eb5a19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Features.pckl', 'rb')\n",
    "features = pickle.load(f)\n",
    "f.close()\n",
    "X = features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24b5d715-0bad-45c4-becb-f2f452839c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Feature_names.pckl', 'rb')\n",
    "feature_names = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff4e021-c4de-483f-9599-c891153280a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Feature_scales.pckl', 'rb')\n",
    "feature_scales = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2977a24-77b8-4eca-8e43-7f84a8e3a8cb",
   "metadata": {},
   "source": [
    "## 2.2. Select scales and features\n",
    "The scales below will be used for the pixel classification. These are appropriate for most geophysical datasets. You can add or remove scales by modifying the code below. If you use a pretrained classifier, please use the same scales and features as used for training the classifier.<br> If an error message appears when calculating the features saying that 'the kernel is longer than the line', this may indicate that one or more scales are too large for the data. This can happen for example in the case of 3D data, when the kernel (which is then also 3D) becomes too large in comparison with the number of horizontal slices. In that case, please remove the largest scale(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3298ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [0.7, 1.0, 1.6, 3.5, 5.0, 10.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4fc6d-869a-48c2-9db4-32779ee6e1af",
   "metadata": {},
   "source": [
    "The image features below will be used for the pixel classification. These are appropriate for most geophysical datasets. You can deselect features by deleting them or commenting them out by putting a '#' at the beginning of the line. Other features can be added and appended to the feature stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "766dbac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFloat = image.astype('float64')\n",
    "feature_stack = []\n",
    "feature_names = []\n",
    "feature_scales = []\n",
    "\n",
    "# Gaussian smoothing with sigma = 0.3\n",
    "gaussian_smoothing = vigra.filters.gaussianSmoothing(image, 0.3)\n",
    "feature_stack.append(gaussian_smoothing.ravel())\n",
    "feature_names.append('Gaussian smoothing')\n",
    "feature_scales.append(0.3)\n",
    "\n",
    "for sigma in sigmas:   \n",
    "    # Gaussian smoothing\n",
    "    gaussian_smoothing = vigra.filters.gaussianSmoothing(imageFloat, sigma)\n",
    "    feature_stack.append(gaussian_smoothing.ravel()) \n",
    "    feature_names.append('Gaussian smoothing')\n",
    "    feature_scales.append(sigma)\n",
    "\n",
    "    # Laplacian of Gaussian\n",
    "    laplacian_of_gaussian = vigra.filters.laplacianOfGaussian(imageFloat, sigma)\n",
    "    feature_stack.append(laplacian_of_gaussian.ravel()) \n",
    "    feature_names.append('Laplacian of Gaussian')\n",
    "    feature_scales.append(sigma)\n",
    "\n",
    "    # Gaussian Gradient Magnitude\n",
    "    gaussian_gradient_magnitude = vigra.filters.gaussianGradientMagnitude(imageFloat, sigma)\n",
    "    feature_stack.append(gaussian_gradient_magnitude.ravel()) \n",
    "    feature_names.append('Gaussian gradient magnitude')\n",
    "    feature_scales.append(sigma)\n",
    "\n",
    "    # Difference of Gaussians\n",
    "    difference_of_gaussians = vigra.filters.gaussianSmoothing(imageFloat, sigma) - vigra.filters.gaussianSmoothing(imageFloat, sigma * 0.66)\n",
    "    feature_stack.append(difference_of_gaussians.ravel())\n",
    "    feature_names.append('Difference of Gaussians')\n",
    "    feature_scales.append(sigma)\n",
    "\n",
    "    # Stucture tensor eigenvalues\n",
    "    if nrofdim == 2:\n",
    "        structure_tensor_eigenvalues = vigra.filters.structureTensorEigenvalues(imageFloat, sigma, sigma * 0.5)[:,:,0]\n",
    "    else:\n",
    "        structure_tensor_eigenvalues = vigra.filters.structureTensorEigenvalues(imageFloat, sigma, sigma * 0.5)[:,:,:,0]\n",
    "    feature_stack.append(structure_tensor_eigenvalues.ravel())\n",
    "    feature_names.append('Structure tensor eigenvalues')\n",
    "    feature_scales.append(sigma)\n",
    "\n",
    "    #Hessian of Gaussian eigenvalues\n",
    "    if nrofdim == 2:\n",
    "        hessian_of_gaussian_eigenvalues = vigra.filters.hessianOfGaussianEigenvalues(imageFloat, sigma)[:,:,1]\n",
    "    else:\n",
    "        hessian_of_gaussian_eigenvalues = vigra.filters.hessianOfGaussianEigenvalues(imageFloat, sigma)[:,:,:,1]\n",
    "    feature_stack.append(hessian_of_gaussian_eigenvalues.ravel())    \n",
    "    feature_names.append('Hessian of Gaussian eigenvalues')\n",
    "    feature_scales.append(sigma)\n",
    "features = np.asarray(feature_stack)\n",
    "X = features.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad762c-f6d1-496b-8990-b3318f55a2c0",
   "metadata": {},
   "source": [
    "## 2.3. Save features\n",
    "If further classification tests will be carried out on the same input data in the future, it may be useful to store pixel features,  feature names and scales for later use. Please adjust the file names or path names in the first line of each cell if necessary (in the code below, files are saved in the same folder as the Jupyter Notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bbbb3a5-99a0-4b4b-890c-73fbfec0aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Features.pckl', 'wb')\n",
    "pickle.dump(features, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33f424b9-045b-4777-8e44-cc69968d46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Feature_names.pckl', 'wb')\n",
    "pickle.dump(feature_names, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60a53a41-44cd-456c-a373-db61e3b552d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Feature_scales.pckl', 'wb')\n",
    "pickle.dump(feature_scales, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785bb8f",
   "metadata": {},
   "source": [
    "## 2.4. View features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4dd34-ec01-463a-9ca2-010b23b63c24",
   "metadata": {},
   "source": [
    "You can create an overview table of the features and scales by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eecc517-1b49-46ef-9b48-c29b3f7cd378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Feature name  Scale\n",
      "0                Gaussian smoothing    0.3\n",
      "1                Gaussian smoothing    0.7\n",
      "2             Laplacian of Gaussian    0.7\n",
      "3       Gaussian gradient magnitude    0.7\n",
      "4           Difference of Gaussians    0.7\n",
      "5      Structure tensor eigenvalues    0.7\n",
      "6   Hessian of Gaussian eigenvalues    0.7\n",
      "7                Gaussian smoothing    1.0\n",
      "8             Laplacian of Gaussian    1.0\n",
      "9       Gaussian gradient magnitude    1.0\n",
      "10          Difference of Gaussians    1.0\n",
      "11     Structure tensor eigenvalues    1.0\n",
      "12  Hessian of Gaussian eigenvalues    1.0\n",
      "13               Gaussian smoothing    1.6\n",
      "14            Laplacian of Gaussian    1.6\n",
      "15      Gaussian gradient magnitude    1.6\n",
      "16          Difference of Gaussians    1.6\n",
      "17     Structure tensor eigenvalues    1.6\n",
      "18  Hessian of Gaussian eigenvalues    1.6\n",
      "19               Gaussian smoothing    3.5\n",
      "20            Laplacian of Gaussian    3.5\n",
      "21      Gaussian gradient magnitude    3.5\n",
      "22          Difference of Gaussians    3.5\n",
      "23     Structure tensor eigenvalues    3.5\n",
      "24  Hessian of Gaussian eigenvalues    3.5\n",
      "25               Gaussian smoothing    5.0\n",
      "26            Laplacian of Gaussian    5.0\n",
      "27      Gaussian gradient magnitude    5.0\n",
      "28          Difference of Gaussians    5.0\n",
      "29     Structure tensor eigenvalues    5.0\n",
      "30  Hessian of Gaussian eigenvalues    5.0\n",
      "31               Gaussian smoothing   10.0\n",
      "32            Laplacian of Gaussian   10.0\n",
      "33      Gaussian gradient magnitude   10.0\n",
      "34          Difference of Gaussians   10.0\n",
      "35     Structure tensor eigenvalues   10.0\n",
      "36  Hessian of Gaussian eigenvalues   10.0\n"
     ]
    }
   ],
   "source": [
    "df = pandas.DataFrame(list(zip(feature_names, feature_scales)), columns = ['Feature name', 'Scale'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f5c38-a21a-4d33-8a3b-48a986da418c",
   "metadata": {},
   "source": [
    "You can view a selected feature by entering its number between the brackets in the first line of the cell below. The number can be found by looking up the feature name and scale in the table above. If the data is 3D, please also enter the horizontal slice to be viewed in the cell below. The image with the selected feature will open in a separate window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81824e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2751b8e52e0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedFeature = features[20,:] # Enter number of feature to be viewed\n",
    "if nrofdim == 2:\n",
    "    selectedFeatureReshaped = np.reshape(selectedFeature,image.shape)\n",
    "else:\n",
    "    selectedSlice = 15 # For 3D data, enter slice to be viewed\n",
    "    selectedFeatureSlice = selectedFeature[(selectedSlice * image.shape[1] * image.shape[2]):((selectedSlice + 1) * image.shape[1] * image.shape[2])]\n",
    "    selectedFeatureReshaped = np.reshape(selectedFeatureSlice,image.shape[1:3])\n",
    "selectedFeatureClip = np.clip(selectedFeatureReshaped,np.mean(selectedFeatureReshaped) - 3 * np.std(selectedFeatureReshaped),np.mean(selectedFeatureReshaped) + 3 * np.std(selectedFeatureReshaped))\n",
    "plt.imshow(selectedFeatureClip,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac144c0-ee78-4c56-8f4e-095a1a9471cb",
   "metadata": {},
   "source": [
    "## 2.5. Load classifier\n",
    "If a classifier has been previously trained and saved, it can be loaded here. Please specify its name in the code below. It should be in the same folder as the Jupyter Notebook, or its path should be specified in the code below. After loading the classifier, please go to step 2.6.2 to apply it to the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f27658fa-28c7-40f0-b55e-95fc55de3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Pixel_classifier.pckl', 'rb')\n",
    "classifier = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9847b-14b7-4aeb-86d7-fc818dfc9b1e",
   "metadata": {},
   "source": [
    "## 2.6. Iterative training and segmentation\n",
    "### 2.6.1. Training\n",
    "If no classifier has been loaded, please first run the code below if a file with labels (in H5 format) is available (for example if the data have been labeled in other software such as ilastik). Please adjust the file name in the first line if necessary. The file should be in the same folder as the Jupyter Notebook, or its path should be specified in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7b8ab5a2-19b5-4b2e-a959-706b87cdcef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Labels.h5\"\n",
    "with h5py.File(filename, \"r\") as f:    \n",
    "    key = list(f.keys())[0]     \n",
    "    if nrofdim == 2:\n",
    "        training_labels = f[key][:,:,0]\n",
    "    else:\n",
    "        training_labels = f[key][:,:,:,0]     \n",
    "    viewer.add_labels(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b49a4-6433-46b2-af9d-9849db533110",
   "metadata": {},
   "source": [
    "If no file with labels is available, please add the 'training_labels' layer to napari by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c105f34b-2b56-4ced-abcb-954b2f578249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'training_labels' at 0x1b980952d60>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels = np.zeros(image.shape[:3], dtype=np.uint8)\n",
    "viewer.add_labels(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d8979",
   "metadata": {},
   "source": [
    "For the iterative training and classification, please follow the procedure below.\n",
    "1. Unless labels have been imported (see the first cell of section 2.6.1 above), train the classifier by selecting the 'training_labels' layer in napari and adding labels to it, using the paint brush and other instruments (see the icons in the top left corner of the viewer under 'Layer controls'). This notebook requires two classes at the stage of pixel classification: background (= label 1) and archaeological structures (= label 2). Further classification occurs at the object level (see Section 3). Nevertheless, in theory it is possible to add more classes at this stage.  \n",
    "2. When the labels have been added, run the cells below to train the classifier and carry out the segmentation (step 2.6.2). After the segmentation has been completed, the result of the segmentation is automatically added as the 'segmentation' layer in napari. \n",
    "3. Additional labels can then be added in napari (in the 'training_labels' layer), and training and segmentation can be carried out again by running the steps below, in an iterative way, until the result of the segmentation is satisfactory.\n",
    "4. The labels can be saved by selecting the 'training_labels' layer and applying 'File -> Save Selected Layer(s)' in napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0ff981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'segmentation' in viewer.layers: \n",
    "    viewer.layers.remove('segmentation')\n",
    "training_labels = viewer.layers['training_labels'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c70bec20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = training_labels.ravel()\n",
    "mask = y > 0\n",
    "Xsel = X[mask]\n",
    "ysel = y[mask]\n",
    "classifier = RandomForestClassifier(n_jobs=-1)\n",
    "classifier.fit(Xsel, ysel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de356b-34e4-4670-8847-9a15676caf6f",
   "metadata": {},
   "source": [
    "### 2.6.2. Segmentation\n",
    "Please run the code below to apply the trained classifier to the input data. The results of the segmentation are added as the 'segmentation' layer in napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af073562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'segmentation' at 0x2751b8b4cd0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classifier.predict(X)\n",
    "segmentation = result.reshape(image.shape)\n",
    "viewer.add_labels(segmentation, opacity = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db3c38-f6eb-4135-966a-ed8f0b632ce4",
   "metadata": {},
   "source": [
    "## 2.7. Save classifier\n",
    "To save the classifier, please specify its name below. In the default code below, it is saved in the same folder as the Jupyter Notebook. Otherwise, its path should be specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b641cb2b-69a9-4d02-8d5d-e3f543043016",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Pixel_classifier.pckl', 'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df342b",
   "metadata": {},
   "source": [
    "## 2.8. Apply hysteresis threshold and label the resulting binary image\n",
    "### 2.8.1. Calculation of probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939857b1-35b8-4f49-9834-554b829cb3ae",
   "metadata": {},
   "source": [
    "In the segmentation in step 2.6.2, the class with the highest probability is assigned to each pixel. To apply hysteresis thresholding to the probability map of a certain class, probabilities need to be calculated first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c03e7047",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = classifier.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e5a85-04cb-44bb-b464-db3712b197b5",
   "metadata": {},
   "source": [
    "The code below adds the probability maps to the napari viewer (for the two classes 'background' and 'archaeological structures' as described in section 2.6.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f765672-a79d-485b-b29c-6f8a40d79fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'Probability (class 2)' at 0x2751b8f7520>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability map for the first class ('background')\n",
    "probabilitymap = probabilities[:,0].reshape(image.shape)\n",
    "viewer.add_image(probabilitymap, opacity = 1.0, name = \"Probability (class 1)\")\n",
    "\n",
    "# Probability map for the second class ('archaeological structures')\n",
    "probabilitymap = probabilities[:,1].reshape(image.shape)\n",
    "viewer.add_image(probabilitymap, opacity = 1.0, name = \"Probability (class 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550d6b61-566e-438e-9666-cc12d2cf2aa5",
   "metadata": {},
   "source": [
    "### 2.8.2. Hysteresis thresholding\n",
    "For the application of hysteresis thresholding, there are two thresholds. Pixels that belong to the 'archaeological structures' class with a probability above the low threshold are only selected if they are connected to pixels above a higher, more stringent probability threshold (the high threshold). Both thresholds can be set in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2587f07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'Segmentation after hysteresis thresholding (Class 2)' at 0x2751bf2ee20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_probabilities = probabilities[:,1].reshape(image.shape)\n",
    "smoothed_probabilities = vigra.filters.gaussianSmoothing(reshaped_probabilities, 0.5)\n",
    "low = 0.5     # Enter the low threshold\n",
    "high = 0.6   # Enter the high threshold\n",
    "hyst = filters.apply_hysteresis_threshold(smoothed_probabilities, low, high)\n",
    "viewer.add_labels(hyst, name = \"Segmentation after hysteresis thresholding (Class 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06985bdf-8115-4f8d-a8eb-f52042db7d2c",
   "metadata": {},
   "source": [
    "### 2.8.3. Creation of individual objects\n",
    "Regions are labelled so that they can be used for subsequent object classification. Pixels receive the same label if they are neighbours in horizontal, vertical or diagonal direction. Connectivity can be changed to '1' if regions with the same label should only consist of pixels connected in horizontal or vertical (and not in diagonal) direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f79e9d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'Objects (Class 2)' at 0x2751d18fb80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_hyst = label(hyst, connectivity = 2, background = 0)\n",
    "viewer.add_labels(labeled_hyst, name = \"Objects (Class 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad6f0b-4ea2-463d-8e21-c843dd7e0481",
   "metadata": {},
   "source": [
    "# 3. Object classification\n",
    "## 3.1. Import .h5 file with segmented objects\n",
    "If the pixel classification has been done in another software package (e.g. ilastik), please import the H5 file with the segmentation by running the code below. Please adjust the file name in the first line if necessary. The file should be in the same folder as the Jupyter Notebook, or its path should be specified in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bb6a4416-d21a-493e-a1ca-8395052f0931",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Binary_image.h5\"\n",
    "with h5py.File(filename, \"r\") as f:    \n",
    "    key = list(f.keys())[0]     \n",
    "    if nrofdim == 2:\n",
    "        labeled_hyst = f[key][:,:,0]\n",
    "    else:\n",
    "        labeled_hyst = f[key][:,:,:,0]      \n",
    "    viewer.add_labels(labeled_hyst, name = \"Segmentation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "592be25f-384b-4afa-9129-2f58fa2e755b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint32')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_hyst.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f734523-2dde-42df-a06e-48838699a7ed",
   "metadata": {},
   "source": [
    "## 3.2. Watershed segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399fb4c4-5165-4a10-86b7-85e2da490f34",
   "metadata": {},
   "source": [
    "Often, the objects are too big for straightforward classification, because they encompass pixels belonging to more than one semantic class (for example parts of a wall and a floor). Watershed is applied to reduce the size of the regions and make them more semantically homogeneous. The size of the resulting regions can be controlled by the size of the footprint in the code below. If no watershed is to be used, please go immediately to the last cell of this section.<br>\n",
    "The code is based on: https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_watershed.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c409bc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'labeled_hyst_watershed' at 0x2751bc5ca90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_hyst = labeled_hyst.astype(np.int32)\n",
    "distance = ndi.distance_transform_edt(labeled_hyst.astype(np.int32))\n",
    "if nrofdim == 2:    \n",
    "    coords = peak_local_max(distance, footprint=np.ones((30,30)), labels=labeled_hyst)     \n",
    "else:\n",
    "    coords = peak_local_max(distance, footprint=np.ones((20,20,20)), labels=labeled_hyst)\n",
    "mask = np.zeros(distance.shape, dtype=bool)\n",
    "mask[tuple(coords.T)] = True\n",
    "markers, _ = ndi.label(mask)\n",
    "labeled_hyst_watershed = watershed(-distance, markers, mask=labeled_hyst)\n",
    "viewer.add_labels(labeled_hyst_watershed, opacity = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66937185-9d74-4424-b59b-508ae701bff6",
   "metadata": {},
   "source": [
    "If no watershed is used, please run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8b0fa8c-b5a3-48ff-9c5f-10e9d1d66210",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_hyst_watershed = labeled_hyst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef913b-7e6a-4bbd-ab3d-d98e0818b06a",
   "metadata": {},
   "source": [
    "## 3.3. Machine learning based object classification\n",
    "### 3.3.1. Selection of object features\n",
    "First run the code below to calculate region properties and create the object feature stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "053e8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = regionprops(labeled_hyst_watershed, intensity_image=image)\n",
    "object_feature_stack = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7da13-b871-4e37-a4b4-c5cba088347a",
   "metadata": {},
   "source": [
    "A large number of object features exist, see the list in https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops.<br> Not all features are supported for 3D data. If you are analysing 3D data, please run the code below for lists of supported and unsupported 3-D features (the code is based on: https://scikit-image.org/skimage-tutorials/lectures/three_dimensional_image_processing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43af6d73-f0a4-4452-9399-1305cc73588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported properties:\n",
      "  area\n",
      "  area_bbox\n",
      "  area_convex\n",
      "  area_filled\n",
      "  axis_major_length\n",
      "  axis_minor_length\n",
      "  bbox\n",
      "  centroid\n",
      "  centroid_local\n",
      "  centroid_weighted\n",
      "  centroid_weighted_local\n",
      "  coords\n",
      "  eccentricity\n",
      "  equivalent_diameter_area\n",
      "  euler_number\n",
      "  extent\n",
      "  feret_diameter_max\n",
      "  image\n",
      "  image_convex\n",
      "  image_filled\n",
      "  image_intensity\n",
      "  inertia_tensor\n",
      "  inertia_tensor_eigvals\n",
      "  intensity_max\n",
      "  intensity_mean\n",
      "  intensity_min\n",
      "  label\n",
      "  moments\n",
      "  moments_central\n",
      "  moments_hu\n",
      "  moments_normalized\n",
      "  moments_weighted\n",
      "  moments_weighted_central\n",
      "  moments_weighted_hu\n",
      "  moments_weighted_normalized\n",
      "  orientation\n",
      "  perimeter\n",
      "  perimeter_crofton\n",
      "  slice\n",
      "  solidity\n",
      "\n",
      "Unsupported properties:\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lieven\\miniconda3\\envs\\envLV09\\lib\\site-packages\\skimage\\measure\\_moments.py:326: RuntimeWarning: invalid value encountered in scalar power\n",
      "  nu[powers] = (mu[powers] / scale ** sum(powers)) / (mu0 ** (sum(powers) / nu.ndim + 1))\n"
     ]
    }
   ],
   "source": [
    "supported = [] \n",
    "unsupported = []\n",
    "\n",
    "for s in stats[0]:\n",
    "    try:\n",
    "        stats[0][s]\n",
    "        supported.append(s)\n",
    "    except NotImplementedError:\n",
    "        unsupported.append(s)\n",
    "\n",
    "print(\"Supported properties:\")\n",
    "print(\"  \" + \"\\n  \".join(supported))\n",
    "print()\n",
    "print(\"Unsupported properties:\")\n",
    "print(\"  \" + \"\\n  \".join(unsupported))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf72de8-2c3b-4933-afea-4bdd5d5ad690",
   "metadata": {},
   "source": [
    "A few combinations of object features are given below (for GPR and for magnetometer). Object features can be appended to the feature stack by adding a code line of the same form and including the name of the feature from the list on https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e949cb20-4677-4e58-ac34-28a8b3c1d847",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lieven\\miniconda3\\envs\\envLV09\\lib\\site-packages\\skimage\\measure\\_regionprops.py:433: UserWarning: Failed to get convex hull image. Returning empty image, see error message below:\n",
      "QH6013 qhull input error: input is less than 3-dimensional since all points have the same x coordinate    0\n",
      "\n",
      "While executing:  | qhull i Qt\n",
      "Options selected for Qhull 2019.1.r 2019/06/21:\n",
      "  run-id 792261220  incidence  Qtriangulate  _pre-merge  _zero-centrum\n",
      "  _max-width  9  Error-roundoff 8.1e-15  _one-merge 5.6e-14\n",
      "  _near-inside 2.8e-13  Visible-distance 1.6e-14  U-max-coplanar 1.6e-14\n",
      "  Width-outside 3.2e-14  _wide-facet 9.7e-14  _maxoutside 6.4e-14\n",
      "\n",
      "  return convex_hull_image(self.image)\n",
      "C:\\Users\\Lieven\\miniconda3\\envs\\envLV09\\lib\\site-packages\\skimage\\measure\\_regionprops.py:632: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return self.area / self.area_convex\n",
      "C:\\Users\\Lieven\\miniconda3\\envs\\envLV09\\lib\\site-packages\\skimage\\measure\\_regionprops.py:433: UserWarning: Failed to get convex hull image. Returning empty image, see error message below:\n",
      "QH6154 Qhull precision error: Initial simplex is flat (facet 1 is coplanar with the interior point)\n",
      "\n",
      "While executing:  | qhull i Qt\n",
      "Options selected for Qhull 2019.1.r 2019/06/21:\n",
      "  run-id 792311641  incidence  Qtriangulate  _pre-merge  _zero-centrum\n",
      "  _max-width 11  Error-roundoff 1.4e-14  _one-merge 9.7e-14\n",
      "  _near-inside 4.9e-13  Visible-distance 2.8e-14  U-max-coplanar 2.8e-14\n",
      "  Width-outside 5.6e-14  _wide-facet 1.7e-13  _maxoutside 1.1e-13\n",
      "\n",
      "The input to qhull appears to be less than 3 dimensional, or a\n",
      "computation has overflowed.\n",
      "\n",
      "Qhull could not construct a clearly convex simplex from points:\n",
      "- p5(v4):     0     0     6\n",
      "- p6(v3):     1     0     0\n",
      "- p76(v2):    11     0     2\n",
      "- p0(v1):     0     0     1\n",
      "\n",
      "The center point is coplanar with a facet, or a vertex is coplanar\n",
      "with a neighboring facet.  The maximum round off error for\n",
      "computing distances is 1.4e-14.  The center point, facets and distances\n",
      "to the center point are as follows:\n",
      "\n",
      "center point        3        0     2.25\n",
      "\n",
      "facet p6 p76 p0 distance=    0\n",
      "facet p5 p76 p0 distance=    0\n",
      "facet p5 p6 p0 distance=    0\n",
      "facet p5 p6 p76 distance=    0\n",
      "\n",
      "These points either have a maximum or minimum x-coordinate, or\n",
      "they maximize the determinant for k coordinates.  Trial points\n",
      "are first selected from points that maximize a coordinate.\n",
      "\n",
      "The min and max coordinates for each dimension are:\n",
      "  0:         0        11  difference=   11\n",
      "  1:         0         0  difference=    0\n",
      "  2:         0         6  difference=    6\n",
      "\n",
      "If the input should be full dimensional, you have several options that\n",
      "may determine an initial simplex:\n",
      "  - use 'QJ'  to joggle the input and make it full dimensional\n",
      "  - use 'QbB' to scale the points to the unit cube\n",
      "  - use 'QR0' to randomly rotate the input for different maximum points\n",
      "  - use 'Qs'  to search all points for the initial simplex\n",
      "  - use 'En'  to specify a maximum roundoff error less than 1.4e-14.\n",
      "  - trace execution with 'T3' to see the determinant for each point.\n",
      "\n",
      "If the input is lower dimensional:\n",
      "  - use 'QJ' to joggle the input and make it full dimensional\n",
      "  - use 'Qbk:0Bk:0' to delete coordinate k from the input.  You should\n",
      "    pick the coordinate with the least range.  The hull will have the\n",
      "    correct topology.\n",
      "  - determine the flat containing the points, rotate the points\n",
      "    into a coordinate plane, and delete the other coordinates.\n",
      "  - add one or more points to make the input full dimensional.\n",
      "\n",
      "  return convex_hull_image(self.image)\n",
      "C:\\Users\\Lieven\\miniconda3\\envs\\envLV09\\lib\\site-packages\\skimage\\measure\\_regionprops.py:433: UserWarning: Failed to get convex hull image. Returning empty image, see error message below:\n",
      "QH6154 Qhull precision error: Initial simplex is flat (facet 1 is coplanar with the interior point)\n",
      "\n",
      "While executing:  | qhull i Qt\n",
      "Options selected for Qhull 2019.1.r 2019/06/21:\n",
      "  run-id 792328448  incidence  Qtriangulate  _pre-merge  _zero-centrum\n",
      "  _max-width  7  Error-roundoff 9.6e-15  _one-merge 6.7e-14\n",
      "  _near-inside 3.4e-13  Visible-distance 1.9e-14  U-max-coplanar 1.9e-14\n",
      "  Width-outside 3.9e-14  _wide-facet 1.2e-13  _maxoutside 7.7e-14\n",
      "\n",
      "precision problems (corrected unless 'Q0' or an error)\n",
      "      1 degenerate hyperplanes recomputed with gaussian elimination\n",
      "      1 nearly singular or axis-parallel hyperplanes\n",
      "      1 zero divisors during back substitute\n",
      "      2 zero divisors during gaussian elimination\n",
      "\n",
      "The input to qhull appears to be less than 3 dimensional, or a\n",
      "computation has overflowed.\n",
      "\n",
      "Qhull could not construct a clearly convex simplex from points:\n",
      "- p1(v4):     0     0     1\n",
      "- p5(v3):     0     0     5\n",
      "- p24(v2):     7     0     1\n",
      "- p0(v1):     0     0     0\n",
      "\n",
      "The center point is coplanar with a facet, or a vertex is coplanar\n",
      "with a neighboring facet.  The maximum round off error for\n",
      "computing distances is 9.6e-15.  The center point, facets and distances\n",
      "to the center point are as follows:\n",
      "\n",
      "center point     1.75        0     1.75\n",
      "\n",
      "facet p5 p24 p0 distance=    0\n",
      "facet p1 p24 p0 distance=    0\n",
      "facet p1 p5 p0 distance= -1.8\n",
      "facet p1 p5 p24 distance=    0\n",
      "\n",
      "These points either have a maximum or minimum x-coordinate, or\n",
      "they maximize the determinant for k coordinates.  Trial points\n",
      "are first selected from points that maximize a coordinate.\n",
      "\n",
      "The min and max coordinates for each dimension are:\n",
      "  0:         0         7  difference=    7\n",
      "  1:         0         0  difference=    0\n",
      "  2:         0         5  difference=    5\n",
      "\n",
      "If the input should be full dimensional, you have several options that\n",
      "may determine an initial simplex:\n",
      "  - use 'QJ'  to joggle the input and make it full dimensional\n",
      "  - use 'QbB' to scale the points to the unit cube\n",
      "  - use 'QR0' to randomly rotate the input for different maximum points\n",
      "  - use 'Qs'  to search all points for the initial simplex\n",
      "  - use 'En'  to specify a maximum roundoff error less than 9.6e-15.\n",
      "  - trace execution with 'T3' to see the determinant for each point.\n",
      "\n",
      "If the input is lower dimensional:\n",
      "  - use 'QJ' to joggle the input and make it full dimensional\n",
      "  - use 'Qbk:0Bk:0' to delete coordinate k from the input.  You should\n",
      "    pick the coordinate with the least range.  The hull will have the\n",
      "    correct topology.\n",
      "  - determine the flat containing the points, rotate the points\n",
      "    into a coordinate plane, and delete the other coordinates.\n",
      "  - add one or more points to make the input full dimensional.\n",
      "\n",
      "  return convex_hull_image(self.image)\n",
      "C:\\Users\\Lieven\\miniconda3\\envs\\envLV09\\lib\\site-packages\\skimage\\measure\\_regionprops.py:433: UserWarning: Failed to get convex hull image. Returning empty image, see error message below:\n",
      "QH6154 Qhull precision error: Initial simplex is flat (facet 1 is coplanar with the interior point)\n",
      "\n",
      "While executing:  | qhull i Qt\n",
      "Options selected for Qhull 2019.1.r 2019/06/21:\n",
      "  run-id 792362062  incidence  Qtriangulate  _pre-merge  _zero-centrum\n",
      "  _max-width  3  Error-roundoff 2.7e-15  _one-merge 1.9e-14\n",
      "  _near-inside 9.4e-14  Visible-distance 5.4e-15  U-max-coplanar 5.4e-15\n",
      "  Width-outside 1.1e-14  _wide-facet 3.2e-14  _maxoutside 2.1e-14\n",
      "\n",
      "precision problems (corrected unless 'Q0' or an error)\n",
      "      4 degenerate hyperplanes recomputed with gaussian elimination\n",
      "      4 nearly singular or axis-parallel hyperplanes\n",
      "      4 zero divisors during back substitute\n",
      "      4 zero divisors during gaussian elimination\n",
      "\n",
      "The input to qhull appears to be less than 3 dimensional, or a\n",
      "computation has overflowed.\n",
      "\n",
      "Qhull could not construct a clearly convex simplex from points:\n",
      "- p2(v4):     2     0     0\n",
      "- p1(v3):     1     0     0\n",
      "- p3(v2):     3     0     0\n",
      "- p0(v1):     0     0     0\n",
      "\n",
      "The center point is coplanar with a facet, or a vertex is coplanar\n",
      "with a neighboring facet.  The maximum round off error for\n",
      "computing distances is 2.7e-15.  The center point, facets and distances\n",
      "to the center point are as follows:\n",
      "\n",
      "center point      1.5        0        0\n",
      "\n",
      "facet p1 p3 p0 distance=    0\n",
      "facet p2 p3 p0 distance=    0\n",
      "facet p2 p1 p0 distance=    0\n",
      "facet p2 p1 p3 distance=    0\n",
      "\n",
      "These points either have a maximum or minimum x-coordinate, or\n",
      "they maximize the determinant for k coordinates.  Trial points\n",
      "are first selected from points that maximize a coordinate.\n",
      "\n",
      "The min and max coordinates for each dimension are:\n",
      "  0:         0         3  difference=    3\n",
      "  1:         0         0  difference=    0\n",
      "  2:         0         0  difference=    0\n",
      "\n",
      "If the input should be full dimensional, you have several options that\n",
      "may determine an initial simplex:\n",
      "  - use 'QJ'  to joggle the input and make it full dimensional\n",
      "  - use 'QbB' to scale the points to the unit cube\n",
      "  - use 'QR0' to randomly rotate the input for different maximum points\n",
      "  - use 'Qs'  to search all points for the initial simplex\n",
      "  - use 'En'  to specify a maximum roundoff error less than 2.7e-15.\n",
      "  - trace execution with 'T3' to see the determinant for each point.\n",
      "\n",
      "If the input is lower dimensional:\n",
      "  - use 'QJ' to joggle the input and make it full dimensional\n",
      "  - use 'Qbk:0Bk:0' to delete coordinate k from the input.  You should\n",
      "    pick the coordinate with the least range.  The hull will have the\n",
      "    correct topology.\n",
      "  - determine the flat containing the points, rotate the points\n",
      "    into a coordinate plane, and delete the other coordinates.\n",
      "  - add one or more points to make the input full dimensional.\n",
      "\n",
      "  return convex_hull_image(self.image)\n",
      "C:\\Users\\Lieven\\miniconda3\\envs\\envLV09\\lib\\site-packages\\skimage\\measure\\_regionprops.py:433: UserWarning: Failed to get convex hull image. Returning empty image, see error message below:\n",
      "QH6154 Qhull precision error: Initial simplex is flat (facet 1 is coplanar with the interior point)\n",
      "\n",
      "While executing:  | qhull i Qt\n",
      "Options selected for Qhull 2019.1.r 2019/06/21:\n",
      "  run-id 792530132  incidence  Qtriangulate  _pre-merge  _zero-centrum\n",
      "  _max-width  5  Error-roundoff 4.5e-15  _one-merge 3.1e-14\n",
      "  _near-inside 1.6e-13  Visible-distance 8.9e-15  U-max-coplanar 8.9e-15\n",
      "  Width-outside 1.8e-14  _wide-facet 5.4e-14  _maxoutside 3.6e-14\n",
      "\n",
      "precision problems (corrected unless 'Q0' or an error)\n",
      "      4 degenerate hyperplanes recomputed with gaussian elimination\n",
      "      4 nearly singular or axis-parallel hyperplanes\n",
      "      4 zero divisors during back substitute\n",
      "      4 zero divisors during gaussian elimination\n",
      "\n",
      "The input to qhull appears to be less than 3 dimensional, or a\n",
      "computation has overflowed.\n",
      "\n",
      "Qhull could not construct a clearly convex simplex from points:\n",
      "- p2(v4):     2     0     0\n",
      "- p1(v3):     1     0     0\n",
      "- p5(v2):     5     0     0\n",
      "- p0(v1):     0     0     0\n",
      "\n",
      "The center point is coplanar with a facet, or a vertex is coplanar\n",
      "with a neighboring facet.  The maximum round off error for\n",
      "computing distances is 4.5e-15.  The center point, facets and distances\n",
      "to the center point are as follows:\n",
      "\n",
      "center point        2        0        0\n",
      "\n",
      "facet p1 p5 p0 distance=    0\n",
      "facet p2 p5 p0 distance=    0\n",
      "facet p2 p1 p0 distance=    0\n",
      "facet p2 p1 p5 distance=    0\n",
      "\n",
      "These points either have a maximum or minimum x-coordinate, or\n",
      "they maximize the determinant for k coordinates.  Trial points\n",
      "are first selected from points that maximize a coordinate.\n",
      "\n",
      "The min and max coordinates for each dimension are:\n",
      "  0:         0         5  difference=    5\n",
      "  1:         0         0  difference=    0\n",
      "  2:         0         0  difference=    0\n",
      "\n",
      "If the input should be full dimensional, you have several options that\n",
      "may determine an initial simplex:\n",
      "  - use 'QJ'  to joggle the input and make it full dimensional\n",
      "  - use 'QbB' to scale the points to the unit cube\n",
      "  - use 'QR0' to randomly rotate the input for different maximum points\n",
      "  - use 'Qs'  to search all points for the initial simplex\n",
      "  - use 'En'  to specify a maximum roundoff error less than 4.5e-15.\n",
      "  - trace execution with 'T3' to see the determinant for each point.\n",
      "\n",
      "If the input is lower dimensional:\n",
      "  - use 'QJ' to joggle the input and make it full dimensional\n",
      "  - use 'Qbk:0Bk:0' to delete coordinate k from the input.  You should\n",
      "    pick the coordinate with the least range.  The hull will have the\n",
      "    correct topology.\n",
      "  - determine the flat containing the points, rotate the points\n",
      "    into a coordinate plane, and delete the other coordinates.\n",
      "  - add one or more points to make the input full dimensional.\n",
      "\n",
      "  return convex_hull_image(self.image)\n",
      "C:\\Users\\Lieven\\miniconda3\\envs\\envLV09\\lib\\site-packages\\skimage\\measure\\_regionprops.py:433: UserWarning: Failed to get convex hull image. Returning empty image, see error message below:\n",
      "QH6154 Qhull precision error: Initial simplex is flat (facet 1 is coplanar with the interior point)\n",
      "\n",
      "While executing:  | qhull i Qt\n",
      "Options selected for Qhull 2019.1.r 2019/06/21:\n",
      "  run-id 792530132  incidence  Qtriangulate  _pre-merge  _zero-centrum\n",
      "  _max-width  4  Error-roundoff 5.5e-15  _one-merge 3.9e-14\n",
      "  _near-inside 1.9e-13  Visible-distance 1.1e-14  U-max-coplanar 1.1e-14\n",
      "  Width-outside 2.2e-14  _wide-facet 6.7e-14  _maxoutside 4.4e-14\n",
      "\n",
      "The input to qhull appears to be less than 3 dimensional, or a\n",
      "computation has overflowed.\n",
      "\n",
      "Qhull could not construct a clearly convex simplex from points:\n",
      "- p9(v4):     3     0     4\n",
      "- p1(v3):     1     0     0\n",
      "- p10(v2):     4     0     0\n",
      "- p0(v1):     0     0     1\n",
      "\n",
      "The center point is coplanar with a facet, or a vertex is coplanar\n",
      "with a neighboring facet.  The maximum round off error for\n",
      "computing distances is 5.5e-15.  The center point, facets and distances\n",
      "to the center point are as follows:\n",
      "\n",
      "center point        2        0     1.25\n",
      "\n",
      "facet p1 p10 p0 distance=    0\n",
      "facet p9 p10 p0 distance=    0\n",
      "facet p9 p1 p0 distance=    0\n",
      "facet p9 p1 p10 distance=    0\n",
      "\n",
      "These points either have a maximum or minimum x-coordinate, or\n",
      "they maximize the determinant for k coordinates.  Trial points\n",
      "are first selected from points that maximize a coordinate.\n",
      "\n",
      "The min and max coordinates for each dimension are:\n",
      "  0:         0         4  difference=    4\n",
      "  1:         0         0  difference=    0\n",
      "  2:         0         4  difference=    4\n",
      "\n",
      "If the input should be full dimensional, you have several options that\n",
      "may determine an initial simplex:\n",
      "  - use 'QJ'  to joggle the input and make it full dimensional\n",
      "  - use 'QbB' to scale the points to the unit cube\n",
      "  - use 'QR0' to randomly rotate the input for different maximum points\n",
      "  - use 'Qs'  to search all points for the initial simplex\n",
      "  - use 'En'  to specify a maximum roundoff error less than 5.5e-15.\n",
      "  - trace execution with 'T3' to see the determinant for each point.\n",
      "\n",
      "If the input is lower dimensional:\n",
      "  - use 'QJ' to joggle the input and make it full dimensional\n",
      "  - use 'Qbk:0Bk:0' to delete coordinate k from the input.  You should\n",
      "    pick the coordinate with the least range.  The hull will have the\n",
      "    correct topology.\n",
      "  - determine the flat containing the points, rotate the points\n",
      "    into a coordinate plane, and delete the other coordinates.\n",
      "  - add one or more points to make the input full dimensional.\n",
      "\n",
      "  return convex_hull_image(self.image)\n",
      "C:\\Users\\Lieven\\miniconda3\\envs\\envLV09\\lib\\site-packages\\skimage\\measure\\_regionprops.py:433: UserWarning: Failed to get convex hull image. Returning empty image, see error message below:\n",
      "QH6154 Qhull precision error: Initial simplex is flat (facet 1 is coplanar with the interior point)\n",
      "\n",
      "While executing:  | qhull i Qt\n",
      "Options selected for Qhull 2019.1.r 2019/06/21:\n",
      "  run-id 792546939  incidence  Qtriangulate  _pre-merge  _zero-centrum\n",
      "  _max-width  6  Error-roundoff 8.3e-15  _one-merge 5.8e-14\n",
      "  _near-inside 2.9e-13  Visible-distance 1.7e-14  U-max-coplanar 1.7e-14\n",
      "  Width-outside 3.3e-14  _wide-facet 1e-13  _maxoutside 6.7e-14\n",
      "\n",
      "The input to qhull appears to be less than 3 dimensional, or a\n",
      "computation has overflowed.\n",
      "\n",
      "Qhull could not construct a clearly convex simplex from points:\n",
      "- p1(v4):     1     3     0\n",
      "- p13(v3):     4     0     0\n",
      "- p25(v2):     6     0     0\n",
      "- p0(v1):     0     5     0\n",
      "\n",
      "The center point is coplanar with a facet, or a vertex is coplanar\n",
      "with a neighboring facet.  The maximum round off error for\n",
      "computing distances is 8.3e-15.  The center point, facets and distances\n",
      "to the center point are as follows:\n",
      "\n",
      "center point     2.75        2        0\n",
      "\n",
      "facet p13 p25 p0 distance=    0\n",
      "facet p1 p25 p0 distance=    0\n",
      "facet p1 p13 p0 distance=    0\n",
      "facet p1 p13 p25 distance=    0\n",
      "\n",
      "These points either have a maximum or minimum x-coordinate, or\n",
      "they maximize the determinant for k coordinates.  Trial points\n",
      "are first selected from points that maximize a coordinate.\n",
      "\n",
      "The min and max coordinates for each dimension are:\n",
      "  0:         0         6  difference=    6\n",
      "  1:         0         5  difference=    5\n",
      "  2:         0         0  difference=    0\n",
      "\n",
      "If the input should be full dimensional, you have several options that\n",
      "may determine an initial simplex:\n",
      "  - use 'QJ'  to joggle the input and make it full dimensional\n",
      "  - use 'QbB' to scale the points to the unit cube\n",
      "  - use 'QR0' to randomly rotate the input for different maximum points\n",
      "  - use 'Qs'  to search all points for the initial simplex\n",
      "  - use 'En'  to specify a maximum roundoff error less than 8.3e-15.\n",
      "  - trace execution with 'T3' to see the determinant for each point.\n",
      "\n",
      "If the input is lower dimensional:\n",
      "  - use 'QJ' to joggle the input and make it full dimensional\n",
      "  - use 'Qbk:0Bk:0' to delete coordinate k from the input.  You should\n",
      "    pick the coordinate with the least range.  The hull will have the\n",
      "    correct topology.\n",
      "  - determine the flat containing the points, rotate the points\n",
      "    into a coordinate plane, and delete the other coordinates.\n",
      "  - add one or more points to make the input full dimensional.\n",
      "\n",
      "  return convex_hull_image(self.image)\n"
     ]
    }
   ],
   "source": [
    "# GPR data\n",
    "#object_feature_stack.append(np.asarray([s.eccentricity for s in stats]))\n",
    "#object_feature_stack.append(np.asarray([s.orientation for s in stats]))\n",
    "object_feature_stack.append(np.asarray([s.solidity for s in stats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "994ba51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnetometer data\n",
    "object_feature_stack.append(np.asarray([s.area for s in stats]))\n",
    "object_feature_stack.append(np.asarray([s.eccentricity for s in stats]))\n",
    "object_feature_stack.append(np.asarray([s.solidity for s in stats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0471d0dd-1b17-42d2-a0ea-6b185c2e0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following code can be used to classify small dipoles in magnetometer data\n",
    "areas = np.asarray([s.area for s in stats])\n",
    "intensity_min = np.asarray([s.intensity_min for s in stats])\n",
    "intensity_max = np.asarray([s.intensity_max for s in stats])\n",
    "intensity_diff = intensity_max - intensity_min\n",
    "ratio_intensity_size = intensity_diff / areas\n",
    "object_feature_stack.append(ratio_intensity_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5110712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_features = np.asarray(object_feature_stack)\n",
    "Xobj = object_features.T\n",
    "Xobj[Xobj == np.inf] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e31f9-fc20-40c8-b1e3-49ef10971bf5",
   "metadata": {},
   "source": [
    "## 3.3.2. Load classifier\n",
    "If an object classifier has been previously trained and saved, it can be loaded here. Please specify its name in the code below. It should be in the same folder as the Jupyter Notebook, or its path should be specified in the code below. After loading the classifier, please go immediately to step 3.3.3.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9a0489-4168-4fd3-baad-a19532e0bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Object_classifier.pckl', 'rb')\n",
    "classifierObj = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ddcbee",
   "metadata": {},
   "source": [
    "## 3.3.3. Interactive training and classification\n",
    "If no classifier has been loaded, please first run the code below to add the 'training_labels_object' layer to napari. Afterwards, if you would like to use raster labels for the classification, please go to step 3.3.3.1; if you would like to use vector shapes, please go to step 3.3.3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d579b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'training_labels_object' at 0x2752748a7c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels_object = np.zeros(image.shape[:3], dtype=np.uint8)\n",
    "viewer.add_labels(training_labels_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad162e-2050-4fa8-a536-969c8cba002b",
   "metadata": {},
   "source": [
    "### 3.3.3.1. Creating training data by means of raster labels\n",
    "For the iterative training and classification, please carry out the procedure below.\n",
    "1. Train the classifier by selecting the 'training_labels_object' layer in napari and adding labels to it, and do so for as many classes as needed, using the paint brush and other instruments (see the icons in the top left corner of the viewer under 'Layer controls'). To label an object, it is sufficient that at least one pixel belonging to the object is labeled. If an object has received different labels, the label with the highest class number prevails.\n",
    "2. When the labels have been added, run the code below and then go to steps 3.3.3.3 and 3.3.3.4. \n",
    "3. After running the code in 3.3.3.3 and 3.3.3.4, additional labels can be added in napari (in the 'training_labels_object' layer), and classification can be carried out again by running the code below and in steps 3.3.3.3 and 3.3.3.4, in an iterative way, until the result of the classification is satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9696194",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_object = viewer.layers['training_labels_object'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2c27b-d348-43dd-a5f0-bffb9bc5ec37",
   "metadata": {},
   "source": [
    "### 3.3.3.2. Creating training data by means of vectors (polygons, lines etc.)\n",
    "Please first run the code below to add a 'shapes' layer to napari. Also indicate the class to which the objects covered by the shapes should belong (second line in the code below). All the objects should belong to the same class. If you are analysing 3D data, please also indicate the layer number in which you will label the objects (the layer number can be seen in the bottom right corner of the napari viewer when working with 3D data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "854b43e5-b9a2-49f4-acfd-e274b8be9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_shapes(name = 'shapes')\n",
    "classnumber = 1\n",
    "layer = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e60aed-db6a-428a-b7d4-d1f4c182faea",
   "metadata": {},
   "source": [
    "For the iterative training and classification using polygons, please carry out the procedure below.\n",
    "\n",
    "1. Select the 'shapes' layer in napari and add shapes to it, using the 'Add polygons' icon or other instruments (see the icons in the top left corner of the viewer, under 'Layer controls'). To label an object, it is sufficient that at least one pixel belonging to the object is covered by the shape.\n",
    "2. After having drawn the shapes, please run the code below, which converts the shapes to raster labels and adds them to the 'training_labels_object' layer.\n",
    "3. Additional shapes can be added in napari, using the same workflow: first create a new shapes layer and indicate the class to which the objects should be assigned (code cell above), then draw the shapes, and run the code below.\n",
    "4. Once all shapes have been added, please go to steps 3.3.3.3 and 3.3.3.4.\n",
    "5. After running the code in 3.3.3.3 and 3.3.3.4, additional vector labels can be added in napari, and classification can be carried out again by running the code below and in steps 3.3.3.3 and 3.3.3.4, in an iterative way, until the result of the classification is satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1097f499-1899-4937-ab98-e03a5167a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = viewer.layers['shapes']\n",
    "viewer.layers.remove('shapes')\n",
    "if nrofdim == 2:\n",
    "    labels = shapes.to_labels(image.shape)    \n",
    "else:\n",
    "    labels = np.zeros(image.shape,dtype=np.uint8)\n",
    "    labels[layernumber,:,:] = shapes.to_labels(image.shape[1:3])\n",
    "training_labels_object[labels > 0] = classnumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4a13a-6daa-4cf8-bbf3-8557c89815d7",
   "metadata": {},
   "source": [
    "### 3.3.3.3. Training the classifier\n",
    "After the training data has been created in steps 3.3.3.1 and/or 3.3.3.2., please run the code below to train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73489133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'class_image' in viewer.layers: \n",
    "    viewer.layers.remove('class_image')\n",
    "yobj = training_labels_object.ravel()\n",
    "annotation_stats = regionprops(labeled_hyst_watershed, intensity_image=training_labels_object)\n",
    "annotated_class = np.asarray([s.intensity_max for s in annotation_stats])\n",
    "maskObj = annotated_class > 0\n",
    "XobjSel = Xobj[maskObj]\n",
    "annotated_class_sel = annotated_class[maskObj]\n",
    "classifierObj = RandomForestClassifier(n_jobs=-1)\n",
    "classifierObj.fit(XobjSel, annotated_class_sel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c061a7-895f-465a-9175-a8ab5a422045",
   "metadata": {},
   "source": [
    "### 3.3.3.4. Applying the classifier to the entire dataset\n",
    "After the classification has been completed, the result of the classification is automatically added as the 'class_image' layer in napari. The labels can be saved by selecting the 'training_labels_object' layer and applying 'File -> Save Selected Layer(s)' in napari.\n",
    "The classification can be saved by selecting the 'class_image' layer and applying 'File -> Save Selected Layer(s)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92d3b977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'class_image' at 0x2751d19e3d0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultObj = classifierObj.predict(Xobj)\n",
    "resultObj0 = [0] + resultObj.tolist()\n",
    "numel = len(resultObj0)\n",
    "class_image = np.zeros(image.shape[:3], dtype=np.uint8)\n",
    "for k in range(numel):    \n",
    "    class_image[labeled_hyst_watershed == k] = resultObj0[k]\n",
    "viewer.add_labels(class_image, opacity = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7ae22-0a99-48ad-b341-9b15abd1da59",
   "metadata": {},
   "source": [
    "The following code is faster for 3D data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3e3f1ccd-a64c-4e79-bffa-29c8ca174624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'class_image' at 0x2474b382460>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultObj = classifierObj.predict(Xobj)\n",
    "numel = len(resultObj)\n",
    "class_image = np.zeros(image.shape[:3], dtype=np.uint8)\n",
    "for k in range(numel):  \n",
    "    if resultObj[k] > 0:     \n",
    "        startz = annotation_stats[k].bbox[0]\n",
    "        stopz = annotation_stats[k].bbox[3]\n",
    "        starty = annotation_stats[k].bbox[1]\n",
    "        stopy = annotation_stats[k].bbox[4]\n",
    "        startx = annotation_stats[k].bbox[2]\n",
    "        stopx = annotation_stats[k].bbox[5]\n",
    "        class_image_portion = class_image[startz:stopz,starty:stopy,startx:stopx]\n",
    "        labeled_hyst_watershed_portion = labeled_hyst_watershed[startz:stopz,starty:stopy,startx:stopx]\n",
    "        class_image_portion[labeled_hyst_watershed_portion == k + 1] = resultObj[k]\n",
    "        class_image[startz:stopz,starty:stopy,startx:stopx] = class_image_portion\n",
    "viewer.add_labels(class_image, opacity = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509359e8-4034-4d7e-9e03-7e6f4ec624bb",
   "metadata": {},
   "source": [
    "## 3.3.4. Save classifier\n",
    "To save the classifier, please specify its name below. In the default code below, it is saved in the same folder as the Jupyter Notebook. Otherwise, its path should be specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b6b65ce2-7a73-4d3d-87a6-65a47dfc2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Object_classifier.pckl', 'wb')\n",
    "pickle.dump(classifierObj, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f563e",
   "metadata": {},
   "source": [
    "# 4. Manual classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0455602-786f-4125-b6f3-9a6911e9821c",
   "metadata": {},
   "source": [
    "## 4.1. Manual assignment of all objects to one class\n",
    "If the vast majority of the objects belong to one class, it may be more straightforward to assign them manually to this class than to perform machine learning based object classification (section 3.3). In that case, run the code below and first adjust the class number in the first line. The objects not belonging to this class can then be assigned to their correct class in steps 4.2, 4.3 and 4.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55f418f-2a6f-44df-a7b4-1bcb628397fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "classnumber = 1\n",
    "class_image = np.zeros(image.shape[:3], dtype=np.uint8)\n",
    "class_image[labeled_hyst_watershed > 0] = classnumber\n",
    "viewer.add_labels(class_image, opacity = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d89e3-c3ec-4341-abba-2a8fe61079be",
   "metadata": {},
   "source": [
    "## 4.2. Manual classification of individual objects\n",
    "Objects wrongly classified by the classifier in section 3.3, or manually in section 4.1, can be manually assigned to a different class. Please first run the code below to add the 'manual_labels_object' labels layer to napari. Afterwards, if you would like to use raster labels for the classification, please go to step 4.2.1; if you would like to use vector shapes, please go to step 4.2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7074d200-7f2a-4b5b-a64e-5c70a723c230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'manual_labels_object' at 0x2752fbc4f40>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_labels_object = np.zeros(image.shape[:3], dtype=np.uint8)\n",
    "viewer.add_labels(manual_labels_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24e717-0c9d-497f-9c4d-a9ac0ac2c364",
   "metadata": {},
   "source": [
    "### 4.2.1. By means of raster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a092c4a3-d4dc-467b-8aa9-dc129458ed25",
   "metadata": {},
   "source": [
    "For the manual object classification, please carry out the procedure below.\n",
    "1. Select the 'manual_labels_object' layer in napari and add labels to it, using the paint brush and other instruments (see the icons in the top left corner of the viewer under 'Layer controls'). To label an object, it is sufficient that at least one pixel belonging to the object is labeled. If an object has received different labels, the label with the highest class number prevails.\n",
    "2. After having labelled the objects, please run the code below, and go to step 4.2.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e498851",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_labels_object = viewer.layers['manual_labels_object'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476cb3a-eb50-4ab4-add5-9b11442cf12b",
   "metadata": {},
   "source": [
    "### 4.2.2. By means of vectors (polygons, lines etc.)\n",
    "Please first run the code below to add a 'shapes' layer to napari. Also indicate the class to which the objects covered by the shapes should belong (second line in the code below). All the objects should belong to the same class. When working with 3D data, please also indicate the horizontal slice number in which you label the objects (the slice number can be seen in the bottom right corner of the napari viewer when working with 3D data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0aa779c7-a47a-481e-aa3a-5e7c5ff5995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_shapes(name = 'shapes')\n",
    "classnumber = 1\n",
    "slicenumber = 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20efff16-f898-4e82-9870-ebe317c3e8f7",
   "metadata": {},
   "source": [
    "For the manual object classification using vector labels, please carry out the procedure below. \n",
    "1. Select the 'shapes' layer in napari and add shapes to it, using the 'Add polygons' or other instruments (see the icons in the top left corner of the viewer, under 'Layer controls'). To label an object, it is sufficient that at least one pixel belonging to the object is covered by the shape.\n",
    "2. After having drawn the shapes, please run the code below, which converts the shapes to raster labels and adds them to the 'manual_labels_object' layer.\n",
    "3. Additional shapes can be added in napari, using the same workflow: first create a new 'shapes' layer and indicate the class to which the objects should be assigned (code cell above), then draw the shapes, and run the code below. \n",
    "4. Once all labels have been added, please go to step 4.2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55a8c0f5-f114-4e11-9c7a-24ba80a9510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = viewer.layers['shapes']\n",
    "viewer.layers.remove('shapes')\n",
    "if nrofdim == 2:\n",
    "    labels = shapes.to_labels(image.shape)\n",
    "else:    \n",
    "    labels = np.zeros(image.shape,dtype=np.uint8)\n",
    "    labels[slicenumber,:,:] = shapes.to_labels(image.shape[1:3])\n",
    "manual_labels_object[labels > 0] = classnumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb422f1-bac1-4181-9a84-0c6e783150b5",
   "metadata": {},
   "source": [
    "### 4.2.3. Applying the modifications made\n",
    "The modifications made in step 4.2.1 or 4.2.2 are applied to the 'class_image' layer by running the code below.\n",
    "The labels can be saved by selecting the 'manual_labels_object' layer and applying 'File -> Save Selected Layer(s)' in napari. The classification can be saved by selecting the 'class_image' layer and applying 'File -> Save Selected Layer(s)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a29620f1-706e-464a-bd1b-e13dc51f5374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'class_image' at 0x2751d4e68e0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'class_image' in viewer.layers: \n",
    "    viewer.layers.remove('class_image')    \n",
    "annotation_stats_man = regionprops(labeled_hyst_watershed, intensity_image=manual_labels_object)\n",
    "annotated_class_man = np.asarray([s.intensity_max for s in annotation_stats_man])\n",
    "annotated_class_man0 = [0] + annotated_class_man.tolist()\n",
    "numel = len(annotated_class_man0)\n",
    "for k in range(numel):     \n",
    "    if annotated_class_man0[k] > 0:        \n",
    "        class_image[labeled_hyst_watershed == k] = annotated_class_man0[k]\n",
    "viewer.add_labels(class_image, opacity = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84abdb81-426c-4c4d-a0d4-ee83dec82dba",
   "metadata": {},
   "source": [
    "The following code is faster for 3D data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "5b271d9c-9cb7-45b6-b195-dac71996cced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'class_image [1]' at 0x2476a9b0b20>"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def medianOfNonZeros (regionmask, manual_labels_object):\n",
    "    manual_labels_object_mask = manual_labels_object[regionmask]\n",
    "    if np.mean(manual_labels_object_mask) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.median(manual_labels_object_mask[np.nonzero(manual_labels_object_mask)])\n",
    "annotation_stats_man = regionprops(labeled_hyst_watershed, intensity_image=manual_labels_object, extra_properties=(medianOfNonZeros,))\n",
    "annotated_class_man = np.asarray([s.medianOfNonZeros for s in annotation_stats_man])\n",
    "numel = len(annotated_class_man)\n",
    "for k in range(numel):  \n",
    "    if annotated_class_man[k] > 0:     \n",
    "        startz = annotation_stats_man[k].bbox[0]\n",
    "        stopz = annotation_stats_man[k].bbox[3]\n",
    "        starty = annotation_stats_man[k].bbox[1]\n",
    "        stopy = annotation_stats_man[k].bbox[4]\n",
    "        startx = annotation_stats_man[k].bbox[2]\n",
    "        stopx = annotation_stats_man[k].bbox[5]\n",
    "        class_image_portion = class_image[startz:stopz,starty:stopy,startx:stopx]\n",
    "        labeled_hyst_watershed_portion = labeled_hyst_watershed[startz:stopz,starty:stopy,startx:stopx]\n",
    "        class_image_portion[labeled_hyst_watershed_portion == k + 1] = annotated_class_man[k]\n",
    "        class_image[startz:stopz,starty:stopy,startx:stopx] = class_image_portion\n",
    "viewer.add_labels(class_image, opacity = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edfdd6c-d826-4987-ae5c-b31d7456eb01",
   "metadata": {},
   "source": [
    "## 4.3. Pixel-based correction of existing objects (no modification of background)\n",
    "Besides the manual re-classification of entire objects (section 4.2), parts of objects can be assigned to a different class, by labelling pixels belonging to that object. Please first run the code below to add the 'manual_labels_pixel' layer to napari. Afterwards, if you would like to use raster labels for the classification, please go to step 4.3.1; if you would like to use vector shapes, please go to step 4.3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ea7a5ed1-cd09-4230-ae0e-97e69944c537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'manual_labels_pixel' at 0x2752fa4fbe0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_labels_pixel = np.zeros(image.shape[:3], dtype=np.uint8)\n",
    "viewer.add_labels(manual_labels_pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1048c1-9cb6-41dc-be89-b3057f857814",
   "metadata": {},
   "source": [
    "### 4.3.1. By means of raster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b5e66-1d09-43a8-b075-0ddd59d7157b",
   "metadata": {},
   "source": [
    "For the manual pixel-based correction, please carry out the procedure below.\n",
    "1. Add labels to the 'manual_labels_pixel' layer in napari, using the paint brush and other instruments. This can be done for all existing classes. Only the pixels that belong to an already existing object, will be re-classified. Where the label is painted outside an existing object (i.e. in the background), it remains background. \n",
    "2. When the labels have been added, please run the code below, and go to step 4.3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5612330d-4018-4e87-9a5e-a1de97ba8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_labels_pixel = viewer.layers['manual_labels_pixel'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cee91-6f97-4946-bcd1-ec8965098d7b",
   "metadata": {},
   "source": [
    "In 3D data, the correction will only apply to the horizontal slice which was annotated. If you want to apply it to all slices, please run the code below. Indicate the slice which was annotated in the first line of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "522f7d22-5caa-46fc-ae3c-6f5ac88c06c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'manual_labels_pixel [1]' at 0x24541830460>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slicenumber = 9\n",
    "manual_labels_pixel_layer = manual_labels_pixel[slicenumber,:,:]\n",
    "manual_labels_pixel = np.tile(manual_labels_pixel_layer,[image.shape[0],1,1])\n",
    "viewer.add_labels(manual_labels_pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df902af1-61b0-49f0-b198-9aedeac5f0ea",
   "metadata": {},
   "source": [
    "### 4.3.2. By means of vector polygons\n",
    "Please first run the code below to add a 'shapes' layer to napari. Also indicate the class to which the pixels covered by the shapes should belong (second line in the code below). All re-classified pixels should belong to the same class. When working with 3D data, please also indicate the slice number in which you label the objects (the slice number can be seen in the bottom right corner of the napari viewer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3ffb1c8f-8254-4a59-9683-7824f2e909a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_shapes(name = 'shapes')\n",
    "classnumber = 1\n",
    "slicenumber = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54231233-0990-415d-a9aa-b7910bd8a721",
   "metadata": {},
   "source": [
    "For the manual pixel-based classification using vector labels, please carry out the procedure below.\n",
    "\n",
    "1. Select the 'shapes' layer in napari and add shapes to it, using the 'Add polygons' or other instruments (see the icons in the top left corner of the viewer, under 'Layer controls'). Only the pixels that belong to an already existing object, will be re-classified. Where the shape extends outside an existing object (i.e. in the background), the background remains unchanged.\n",
    "2. After having drawn the shapes, please run the code below, which converts the shapes to raster labels and adds them to the 'manual_labels_pixel' layer.\n",
    "3. In 3D data, the correction will only apply to the horizontal slice which was annotated if the variable 'applyToAllSlices' is set to 'N' (first line of the code below). If you want the correction to apply to all slices, please set this variable to 'Y'.\n",
    "4. Additional shapes can be added in napari, using the same workflow: first create a new shapes layer and indicate the class to which the objects should be assigned (code cell above), then draw the shapes, and run the code below.\n",
    "5. Once all labels have been added, please go to step 4.3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "32abdaf8-7934-4de1-8651-d62b8ff40d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "applyToAllSlices = 'Y' # For 3D data: apply changes only to slice which was annotated ('N') or to all slices ('Y')\n",
    "shapes = viewer.layers['shapes']\n",
    "viewer.layers.remove('shapes')\n",
    "if nrofdim == 2:\n",
    "    labels = shapes.to_labels(image.shape)\n",
    "else:\n",
    "    labels = np.zeros(image.shape,dtype=np.uint8)\n",
    "    labels_slice = shapes.to_labels(image.shape[1:3])\n",
    "    if applyToAllSlices == 'N':\n",
    "        labels[slicenumber,:,:] = labels_slice\n",
    "    else:\n",
    "        labels = np.tile(labels_slice,[image.shape[0],1,1])\n",
    "manual_labels_pixel[labels > 0] = classnumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7300bdd-e07e-453e-87cd-750c7a2e419f",
   "metadata": {},
   "source": [
    "### 4.3.3. Applying the modifications made\n",
    "The modifications made in step 4.3.1 or 4.3.2 are applied to the 'class_image' layer by running the code below. The labels can be saved by selecting the 'manual_labels_pixel' layer and applying 'File -> Save Selected Layer(s)' in napari. The classification can be saved by selecting the 'class_image' layer and applying 'File -> Save Selected Layer(s)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20bd8e13-4ad5-4ef3-a909-873ce93b4da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'class_image' at 0x27530ac6430>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'class_image' in viewer.layers: \n",
    "    viewer.layers.remove('class_image')\n",
    "classMax = np.max(manual_labels_pixel)\n",
    "for k in range(classMax):\n",
    "    class_image[manual_labels_pixel == k + 1] = k + 1    \n",
    "class_image[labeled_hyst_watershed == 0] = 0\n",
    "viewer.add_labels(class_image, opacity = 0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a251e-f409-40fb-ae6d-001d8d7898c3",
   "metadata": {},
   "source": [
    "## 4.4. Pixel-based correction (including background), or creation of new objects\n",
    "Whereas in section 4.3 pixels can be re-classified only if they belong to an existing object, in this section any pixel can be re-classified (including the background) by labelling it. In this way also new objects can be created. \n",
    "1. If you would like to use raster labels for this correction, simply add labels to the 'class_image' layer, using the paint brush and other instruments. Where the label is painted outside existing objects (i.e. in the background class), these pixels are assigned to the new class. It may be useful to save the 'class_image' layer first, by selecting this layer and applying 'File -> Save Selected Layer(s)' in napari (because changes can only be undone manually with the eraser and paint brush).\n",
    "2. If you would like to use vector shapes for the correction, please first run the code below to add a 'shapes' layer to napari. Also indicate the class to which the pixels covered by the shapes should belong (second line in the code below). All re-classified pixels should belong to the same class. When working with 3D data, please also indicate the horziontal slice number in which you label the objects (the slice number can be seen in the bottom right corner of the napari viewer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b43bd02f-f269-4d5a-b7e8-7ee49eb9fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_shapes(name = 'shapes')\n",
    "classnumber = 3\n",
    "slicenumber = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc2ee1-e33d-4944-9aa8-d4714f46b600",
   "metadata": {},
   "source": [
    "For the manual pixel-based correction (including background), please carry out the procedure below.\n",
    "\n",
    "1. Select the 'shapes' layer in napari and add shapes to it, using the 'Add polygons' or other instruments (see the icons in the top left corner of the viewer, under 'Layer controls'). Where the shape is drawn outside existing objects (i.e. in the background class), these pixels are assigned to the new class. In this way, this tool can be used for the creation of new objects.\n",
    "2. After having drawn the shapes, please run the code below, which converts the shapes to raster labels and adds them to the 'class_image' layer.\n",
    "3. Additional shapes can be added in napari, using the same workflow: first create a new shapes layer and indicate the class to which the objects should be assigned (code cell above), then draw the shapes, and run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d379286c-cd6c-45e0-b6c5-22a9a91345bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = viewer.layers['shapes']\n",
    "viewer.layers.remove('shapes')\n",
    "if nrofdim == 2:\n",
    "    labels = shapes.to_labels(image.shape)\n",
    "else:    \n",
    "    labels = np.zeros(image.shape,dtype=np.uint8)\n",
    "    labels[slicenumber,:,:] = shapes.to_labels(image.shape[1:3])    \n",
    "class_image[labels > 0] = classnumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede27e2-584e-46c5-a739-b39af8982577",
   "metadata": {},
   "source": [
    "# 5. Postprocessing\n",
    "## 5.1. Creation of 2-D image from 3-D data\n",
    "When analysing 3D data, it may be useful to project the 3D segmentation onto a 2D map, e.g. for use in a GIS. To create a 2D map from a 3D label volume, please run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "f0207e93-6011-478d-bdfd-07b232e9b949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'class_image_slice' at 0x2475704c2b0>"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_image_slice = np.zeros(image.shape[1:3], dtype=np.uint8)\n",
    "for m in range(image.shape[1]):    \n",
    "    for n in range(image.shape[2]):\n",
    "        class_image_column = class_image[:,m,n]\n",
    "        if np.mean(class_image_column) == 0:\n",
    "            class_image_slice[m,n] = 0            \n",
    "        else:\n",
    "            class_image_slice[m,n] = np.median(class_image_column[np.nonzero(class_image_column)])             \n",
    "viewer.add_labels(class_image_slice, opacity = 0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ab29c-3a7e-47c9-a6cc-2db949d156f6",
   "metadata": {},
   "source": [
    "## 5.2. Creation of separate labels layers for each class\n",
    "A text file with the class names needs to be provided. In this file, the text must be of the form: {\"1\": \"Ditch\", \"2\": \"Road\", \"3\": \"House\"}. In the default code below, this text file is named 'Classes_test.txt'. If it is named differently, please specify its name below. It should be in the same folder as the Jupyter Notebook, or otherwise its path should be specified in the code below.<br> Within the folder where the input data is located, a 'Classes' folder will be created (first code cell below). In that folder, for each class a file with the objects belonging to that class will be stored (second code cell below). When working with 3D data, please indicate in the first line of the second code cell below if the files representing individual classes should be 2D (i.e. extracted from the 2D map created in step 5.1) or 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "afc403a8-4674-4e19-90fa-8ba1ba5672a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classImageMax = int(np.max(class_image))\n",
    "with open(\"Classes_test.txt\",\"r\") as file:\n",
    "    classes = json.loads(file.read())\n",
    "pathname = os.path.dirname(viewer.layers[0].source.path)\n",
    "pathnameClasses = pathname + '/Classes/'\n",
    "pathnameClassesExist = os.path.exists(pathnameClasses)\n",
    "if not pathnameClassesExist:\n",
    "    os.makedirs(pathnameClasses)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b877a44d-3536-4a85-8308-1376a7d3ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filesIn3D = 'Y' # For 3D data: make 2D layers representing individual classes ('N') or 3D layers ('Y') \n",
    "for m in range(len(classes)):\n",
    "    classnr = int(list(classes)[m])\n",
    "    if nrofdim == 2:\n",
    "        objectclass = np.zeros(image.shape, dtype=np.uint8)\n",
    "        objectclass[class_image == classnr] = classnr\n",
    "    else:\n",
    "        if filesIn3D == 'N':\n",
    "            objectclass = np.zeros(image.shape[1:3], dtype=np.uint8)\n",
    "            objectclass[class_image_slice == classnr] = classnr\n",
    "        else:\n",
    "            objectclass = np.zeros(image.shape, dtype=np.uint8)\n",
    "            objectclass[class_image == classnr] = classnr\n",
    "    mstr = str(classnr)\n",
    "    classname = classes[mstr]\n",
    "    viewer.add_labels(objectclass, opacity = 0.8, name = classname)\n",
    "    fullname = pathnameClasses + classname + '.tif'\n",
    "    viewer.layers[classname].save(fullname)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4043b328-78fb-4885-b556-d43e9652aef6",
   "metadata": {},
   "source": [
    "## 5.3. Morhological operations\n",
    "With the code below, morphological operations can be performed. First, indicate the number of the class to which the operation should be applied and the radius of the structuring element. Other structuring elements can be used, see https://scikit-image.org/docs/stable/api/skimage.morphology.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fa594774-52c4-49eb-8f88-bd2426f9ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "classnumber = 3\n",
    "classnrstr = str(classnumber)\n",
    "classname = classes[classnrstr]\n",
    "original = viewer.layers[classname].data\n",
    "radius = 3\n",
    "if nrofdim == 2:\n",
    "    footprint = morphology.disk(radius)\n",
    "else:\n",
    "    footprint = morphology.ball(radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1b238-e615-407d-b79a-edbf3163c0ec",
   "metadata": {},
   "source": [
    "Four operations can be performed: morphological dilation, erosion, closing and opening. For other operations, see https://scikit-image.org/docs/stable/api/skimage.morphology.html. The result of the operation is saved in the 'Classes' folder created in section 5.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7b4f1476-ae49-478e-91d8-84cd27aaf8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Archeologie/Machine_learning/scikit-learn/Figuren_Interamna/3-D/7001-8000_6001-7000/Classes/Walls_morph_dilation.tif']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Morphological dilation\n",
    "processed = morphology.dilation(original, footprint)\n",
    "viewer.add_labels(processed, opacity = 0.8, name = classname + '_morph_dilation')\n",
    "fullnameMorphDilat = pathnameClasses + classname + '_morph_dilation.tif'\n",
    "viewer.layers[classname + '_morph_dilation'].save(fullnameMorphDilat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9df0fa1d-c7e2-4be9-841d-7ce1d80620f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Archeologie/Machine_learning/scikit-learn/Figuren_Boviolles/Volledige_dataset/20241102_test_Jupyter_notebook/Classes/Linear_feature_morph_erosion.tif']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Morphological erosion\n",
    "processed = morphology.erosion(original, footprint)\n",
    "viewer.add_labels(processed, opacity = 0.8, name = classname + '_morph_erosion')\n",
    "fullnameMorphErosion = pathnameClasses + classname + '_morph_erosion.tif'\n",
    "viewer.layers[classname + '_morph_erosion'].save(fullnameMorphErosion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "21601e5d-31e5-4661-809e-4af8cc152218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Archeologie/Machine_learning/scikit-learn/Figuren_Interamna/3-D/7001-8000_6001-7000/Classes/Walls_morph_opening.tif']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Morphological opening\n",
    "processed = morphology.opening(original, footprint)\n",
    "viewer.add_labels(processed, opacity = 0.8, name = classname + '_morph_opening')\n",
    "fullnameMorphOpen = pathnameClasses + classname + '_morph_opening.tif'\n",
    "viewer.layers[classname + '_morph_opening'].save(fullnameMorphOpen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "7c00207a-414a-4731-8022-5b1bc05c5334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:/Archeologie/Machine_learning/scikit-learn/Figuren_Interamna/3-D/7001-8000_6001-7000/Classes/Walls_morph_closing.tif']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Morphological closing\n",
    "processed = morphology.closing(original, footprint)\n",
    "viewer.add_labels(processed, opacity = 0.8, name = classname + '_morph_closing')\n",
    "fullnameMorphClos = pathnameClasses + classname + '_morph_closing.tif'\n",
    "viewer.layers[classname + '_morph_closing'].save(fullnameMorphClos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be618bc-ab54-4769-8211-fd0d1b2df6d7",
   "metadata": {},
   "source": [
    "# 6. Convert raster images to polygons, and save them as shapefiles\n",
    "Please provide the following information, by adapting the code below:\n",
    "1. x-coordinate (Easting) and y-coordinate (Northing) of the lower left corner of the input data;\n",
    "2. sample distance in x- and y-direction (in m);\n",
    "3. coordinate reference system (EPSG code: https://epsg.org/home.html). <br>\n",
    "\n",
    "If the input image is a GeoTIFF, this information is automatically extracted by running the code in the first cell below. Otherwise it should be entered manually in the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "070204a4-9d3d-4323-9212-2cd59b8eda5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is based on: https://stackoverflow.com/questions/79059686/read-xyz-geotiff-file-with-tifffile\n",
    "with tifffile.TiffFile(path) as tif:\n",
    "    tif_tags = {}\n",
    "    for tag in tif.pages[0].tags.values():\n",
    "        name, value = tag.name, tag.value\n",
    "        tif_tags[name] = value\n",
    "coordx = tif_tags['ModelTiepointTag'][3]\n",
    "coordy = tif_tags['ModelTiepointTag'][4] - tif_tags['ImageLength'] * tif_tags['ModelPixelScaleTag'][1]\n",
    "sampledistx = tif_tags['ModelPixelScaleTag'][0]\n",
    "sampledisty = tif_tags['ModelPixelScaleTag'][1]\n",
    "CoordRefSyst = 'EPSG:' + str(tif_tags['GeoKeyDirectoryTag'][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e2a810b9-0d64-4ab2-a25e-670516734be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordx = 395750 #395600 #876899 #876649\n",
    "coordy = 4586831 #4586781 #6840225 #6839975\n",
    "sampledistx = 0.05 #0.25\n",
    "sampledisty = 0.05 #0.25\n",
    "CoordRefSyst = \"EPSG:32633\" #\"EPSG:32631\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20886a-34fb-489a-83fc-8b887bc476c4",
   "metadata": {},
   "source": [
    "To run the code below, a text file mentioning the classes should be provided. In this file, the text should be of the form: {\"1\": \"Ditch\", \"2\": \"Road\", \"3\": \"House\"}. In the default code below, this text file is named 'Classes_test.txt'. If it is named differently, please specify its name below. It should be in the same folder as the Jupyter Notebook, or otherwise its path should be specified in the code below. <br> A subfolder called 'Shapefiles' will be created in the folder where input data is located. For each class, a shapefile with the name of the class is created. The path and file names can be changed by modifying the 'pathname' and 'fullname' variables in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0fb2b13a-fc60-4c32-926c-3608244bc16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Classes_Boviolles2.txt\",\"r\") as file:\n",
    "    classes = json.loads(file.read())\n",
    "pathname = os.path.dirname(viewer.layers[0].source.path)\n",
    "pathnameshp = pathname + '/Shapefiles/'\n",
    "pathnameshpExist = os.path.exists(pathnameshp)\n",
    "if not pathnameshpExist:\n",
    "    os.makedirs(pathnameshp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2da7074c-e923-43e7-a632-33f26180cf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lieven\\AppData\\Local\\Temp\\ipykernel_23244\\1927754597.py:58: FutureWarning: Currently, index_parts defaults to True, but in the future, it will default to False to be consistent with Pandas. Use `index_parts=True` to keep the current behavior and True/False to silence the warning.\n",
      "  gdf_exploded = gdf.explode()\n",
      "C:\\Users\\Lieven\\AppData\\Local\\Temp\\ipykernel_23244\\1927754597.py:58: FutureWarning: Currently, index_parts defaults to True, but in the future, it will default to False to be consistent with Pandas. Use `index_parts=True` to keep the current behavior and True/False to silence the warning.\n",
      "  gdf_exploded = gdf.explode()\n"
     ]
    }
   ],
   "source": [
    "# This code is based on: \n",
    "# https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html\n",
    "# https://docs.opencv.org/4.x/d9/d8b/tutorial_py_contours_hierarchy.html\n",
    "# https://stackoverflow.com/questions/57965493/how-to-convert-numpy-arrays-obtained-from-cv2-findcontours-to-shapely-polygons\n",
    "# https://gis.stackexchange.com/questions/353057/how-to-create-a-shapely-polygon-with-a-hole\n",
    "# https://shapely.readthedocs.io/en/stable/reference/shapely.MultiPolygon.html\n",
    "# https://shapely.readthedocs.io/en/2.0.3/manual.html\n",
    "\n",
    "for r in range(len(classes)):\n",
    "    classnr = list(classes)[r]   \n",
    "    classname = classes[classnr]\n",
    "    classbinarymap = viewer.layers[classname].data\n",
    "    classbinarymap[classbinarymap == int(classnr)] = 1   \n",
    "    classbinarymap2 = np.zeros(classbinarymap.shape, dtype=np.uint8)\n",
    "    \n",
    "    # Label connected components\n",
    "    classbinarymaplabeled = label(classbinarymap, connectivity=1)\n",
    "    \n",
    "    # Compute region properties\n",
    "    stats = regionprops(classbinarymaplabeled)\n",
    "    \n",
    "    # Filter bounding boxes\n",
    "    for q, s in enumerate(stats):\n",
    "        box = s.bbox\n",
    "        if box[2] - box[0] > 1 and box[3] - box[1] > 1:\n",
    "            classbinarymap2[classbinarymaplabeled == q + 1] = 1\n",
    "    \n",
    "    # Find contours\n",
    "    #contours, _ = cv.findContours(classbinarymap2, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    contours, hierarchy = cv.findContours(classbinarymap2, cv.RETR_CCOMP, cv.CHAIN_APPROX_NONE)\n",
    "    #contoursSq = map(np.squeeze, contours)\n",
    "\n",
    "    # Convert contours to polygons. Take into account possible holes in polygons using the hierarchy of the contours\n",
    "    listofpolygons = []\n",
    "    for k in range(len(contours)):\n",
    "    \tif hierarchy[0][k][3] == -1:\n",
    "    \t\texterior = []\n",
    "    \t\tholes = []\n",
    "    \t\tfor m in range(len(contours[k])):\n",
    "    \t\t\ttuple_element = (contours[k][m][0][0],contours[k][m][0][1])\n",
    "    \t\t\texterior.append(tuple_element)\n",
    "    \t\tfor p in range(len(contours)):\n",
    "    \t\t\tif hierarchy[0][p][3] == k:\n",
    "    \t\t\t\tinterior = []\n",
    "    \t\t\t\tfor n in range(len(contours[p])):\n",
    "    \t\t\t\t\ttuple_element = (contours[p][n][0][0],contours[p][n][0][1])\n",
    "    \t\t\t\t\tinterior.append(tuple_element)\n",
    "    \t\t\t\t\tinterior[::-1]\n",
    "    \t\t\t\tholes.append(interior)\n",
    "    \t\tpgn = Polygon(exterior,holes)\n",
    "    \t\tlistofpolygons.append(pgn)\n",
    "    multipolygon = MultiPolygon(listofpolygons)\n",
    "       \n",
    "    # Apply geometric transformations\n",
    "    multipolygonflipud1 = affine_transform(multipolygon,[1, 0, 0, -1, 0, 0])\n",
    "    if nrofdim == 2:\n",
    "        multipolygonflipud2 = translate(multipolygonflipud1, yoff=image.shape[0])\n",
    "    elif nrofdim == 3:\n",
    "        multipolygonflipud2 = translate(multipolygonflipud1, yoff=image.shape[1])\n",
    "    multipolygonscaled = scale(multipolygonflipud2, xfact=sampledistx, yfact=sampledisty, origin=(0,0))    \n",
    "    multipolygontransl = translate(multipolygonscaled, xoff=coordx, yoff=coordy)\n",
    "    \n",
    "    # Create GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(index=[0], geometry=[multipolygontransl], crs=CoordRefSyst)\n",
    "    \n",
    "    # Explode multipolygon into individual geometries\n",
    "    gdf_exploded = gdf.explode()\n",
    "    \n",
    "    # Save shapefile    \n",
    "    fullname = pathnameshp + classname + '.shp'\n",
    "    gdf_exploded.to_file(fullname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
